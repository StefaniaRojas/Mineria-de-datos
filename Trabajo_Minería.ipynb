{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StefaniaRojas/Mineria-de-datos/blob/main/Trabajo_Miner%C3%ADa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color= blue> <b> <center> CLASIFICACIÓN DE VECINOS MÁS CERCANOS (KNN) </b> </i> </font>  </center>"
      ],
      "metadata": {
        "id": "r_fJLxmKYUvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Nota histórica"
      ],
      "metadata": {
        "id": "mNUQ4AxzaAys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Todo empieza con el informe de análisis técnico en 1951 producido para la USAF, escrito por Evelyn Fix(1904–1965) y Hodges Jr. (1922–2000) estadísticos de Berkeley que introdujeron un método de clasificación no paramétrico (análisis discriminante). Probablemente nunca publicaron el documento oficialmente por temas de confidencialidad, teniendo en cuenta el contexto de la guerra fria.\n",
        "\n",
        "Así se daría la primera clasificación no paramétrica que posteriormente el mundo conocería como **algoritmo kNN** un quindemio después en el departamento de ingeníería Electrica de la universidad Stanford se publicaría el articulo \"Nearest neighbor pattern classification\" aquí Thomas Cover y Peter Hart demostrarían que la mitad de la información de clasificación en un conjunto de muestras infinitas está contenida en el vecino más cercano, argumentando que.\n",
        "\n",
        "> \"Estos límites son los más estrechos posibles, para todas las distribuciones subyacentes adecuadamente uniformes. Así, para cualquier número de categorías, la probabilidad de error de la regla del vecino más cercano está acotada arriba por el doble de la probabilidad de error de Bayes\"(1967,P.Hart,p1).\n",
        "\n",
        "En las descadas subsecuentes se han implementado muchas mejoras y que seguramente seguirán implementandose. \n",
        "\n",
        "Una idea más clara de lo que sería el algortimo de KNN sería.\"El algoritmo kNN clasifica un punto de datos según la clasificación de sus vecinos. Asume que existen cosas similares en la proximidad, y depende de que esta suposición sea lo suficientemente cierta como para que el algoritmo sea útil\",(dataiku,2021)\n",
        "pese a que su fundamento es simple, lograría ser uno de los mejores clasificadores dejando atrás a otros más potentes, por lo cual se utliza en diferentes areas, es decir, es un algoritmo multifuncional, además en algunas ocasiones se usa como base para la construccion de algortimos más robustos.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m2AkpA6BZX68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Introducción"
      ],
      "metadata": {
        "id": "218LNwaUZFuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algunos de los aspectos más importante a tener en cuenta es que al ser no parametrico no hace suposiciones sobre los datos, además opta por recopilar información sobre su estructura y así utilizarla como información apriorí para \n",
        "las predicciones.\n",
        "\n",
        "Por lo anterior a pesar de que KNN suele ser muy intuitivo, descubre patrones y es muy preciso, también tiene problemas cuando se trata del manejo de grandes bases de datos y más si son de gran dimensionalidad, además de ser sensible con respecto a datos atipicos o faltantes.\n"
      ],
      "metadata": {
        "id": "Oxty0BFecTUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funcionamiento \n",
        "\n",
        "Parte de la idea del algoritmo es calcular distancias con respecto a una clasificación y por tanto encontrar sus vecinos más cercanos mediante el voto de etiquetas, para ello se podría hablar de cuatro tipos de distancia según el entorno.\n",
        "\n",
        "\n",
        "*   Distancia Euclidiana \n",
        "*   Distancia Manhattan\n",
        "*   Distancia Mahalanobis\n",
        "*   Distancia Minkowski\n",
        "\n",
        "En cuanto a la clasificación el hiperparámetro *k* sería aquella variable de control para el modelo de predicción. Teniendo en cuenta que dependiendo la naturaleza de los datos dependerá el número preciso de vecinos, lo cual sería directamente proporcional al sesgo pero inversamente proporcional a la varianza, como ultima recomendación si el numero de clases es impar eliga un numero par.\n",
        "\n"
      ],
      "metadata": {
        "id": "IFMxRJo6IgLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso a paso\n",
        "\n",
        "Para mirar un poco más en detalle como sería la metodología de este algoritmo dataiku (2021) nos muestra un paso a paso a seguir.\n",
        "\n",
        "1.   Importe las bibliotecas de Python relevantes\n",
        "2.   Importar los datos\n",
        "3.   Leer/limpiar/ajustar los datos (si es necesario)\n",
        "4.   Crear una división de entrenamiento/prueba\n",
        "5.   Crear el objeto modelo kNN\n",
        "6.   Ajuste el modelo\n",
        "7.   Predecir\n",
        "8.   Evaluar la precisión\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "guCJ3MeWcYEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computación\n",
        "Como se mencionó anteriormente, este algoritmo se caracteriza por tener una metodología simple, no obstante es un metodo de clasificación bastante competitivo e importante en temas de  minería de datos y reconocimiento de patrones. A pesar de algunas restricciones en cuanto al rendimiento es bastante común y popular para trabajar en un conjunto de datos. Es de notar que existen parámetros de optimización para el algoritmo, un ejemplo de ello sería en la función de python \"KNeighborsClassifier\", la cual tendría los siguiente parametros:\n",
        "\n",
        "*   n_neighbor: (predeterminado 5 ) Regula cuántos vecinos se deben revisar cuando se clasifica un artículo.\n",
        "*   pesos:  (predeterminado: \" uniforme \") este valor hará que los pesos se distribuyan por igual entre todos los valores vecinos. \n",
        "*   algoritmo:  (predeterminado: “auto”) utiliza el algoritmo más adecuado automáticamente en función del conjunto de datos.\n",
        "\n",
        "Es de notar, que además de esto existen otros parametrós de optimización kNN para un ajuste fino, es decir, tener una mayor optimización, además de evitar ineficiencias de rendimiento y tamaño.\n",
        "\n",
        "\n",
        "*  tamaño_hoja (predeterminado: 30 ) Si se eligen los algoritmos BallTree o KDTree, esto permitirá el uso de parámetros adicionales como leaf_size, metrics, metric_size.\n",
        "*  pags (predeterminado: 2 ) El parámetro p significa la potencia para Minkowski; manhattan_distance (l1) y euclidean_distance (l2)\n",
        "*  n_trabajos (predeterminado: Ninguno ) Significa que los trabajos paralelos se permitirán al mismo tiempo para el algoritmo vecino Ninguno: asigna 1 como valor.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dl-lL70pNASY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Fundamento matemático"
      ],
      "metadata": {
        "id": "WpaJKwV2aIF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora veamos más gráficamente de lo que se trata este método. Suponga que x es el punto que se quiere predecir. Para poder realizar esto, primero se encuentra el punto más cercano a este valor y así se clasifican los puntos para obtener el voto mayoritario de los vecinos. Luego los objetos votan por la clase y la que tenga más votos se toma como la predicción. Y finalmente, para encontrar los puntos similares más cercanos, se busca la distancia entre ellos usando algún método para encontrar distancias.\n",
        "\n",
        "Fuente: [Aprendeia](https://aprendeia.com/algoritmo-k-vecinos-mas-cercanos-teoria-machine-learning/#:~:text=K%20vecinos%20m%C3%A1s%20cercanos%20es,y%20la%20detecci%C3%B3n%20de%20intrusos)"
      ],
      "metadata": {
        "id": "EadzK000AlAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para la realización de este algoritmo se puede hacer uso de diferentes métodos para calcular las distancias entre los valores deseados. Se hablará primeramente de las formas clásicas de calcular estas distancias, para luego mencionar otros métodos comúnes."
      ],
      "metadata": {
        "id": "-C8fD61J80oj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distancia Euclidiana\n",
        "La distancia que existe entre dos puntos en el plano, es considerada como la distancia euclidiana. Esto se puede deducir del teorema de pitagoras. \n",
        "\n",
        "Supongamos que se tiene que $(x_1,y_1)$ y $(x_2,y_2)$ son dos puntos en un plano bidimensional. Se tiene entonces que la distancia euclidiana esta definida, teniendo en cuenta el teorema de pitagoras como:\n",
        "$$d=\\sqrt{\\left(x_2-x_1\\right)^2+\\left(y_2-y_1\\right)^2}$$\n",
        "\n",
        "Fuente: [CUEMATH](https://www.cuemath.com/euclidean-distance-formula/)\n",
        "\n",
        "Teniendo que \n",
        "\n",
        "*   $(x_1,y_1)$ es la coordenada del primer punto\n",
        "*   $(x_2,y_2)$ es la coordenada del segundo punto\n",
        "*   $d$ la distancia entre los dos puntos\n",
        "\n",
        "\n",
        "Por lo que de manera general se tiene que la distancia euclidiana se puede calcular como,\n",
        "\n",
        "$$d=\\sqrt{\\sum_{i=1}^k\\left(x_i-y_i\\right)^2}$$"
      ],
      "metadata": {
        "id": "LxwhhOo2FPj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distancia Manhattan\n",
        "La distancia que existe entre dos puntos o vectores, teniendo en cuenta la suma de las diferencias absolutas $|X-Y|$, es considerada la distancia de Manhattan o distancia de Taxicab o distancia de City Block, debido a que se ve como una cuadrícula, que muestra como ir en diferentes direcciones a través de las calles de la ciudad. \n",
        "\n",
        "Supongamos que se tienen $X$ y $Y$ puntos en un espacio euclidiano de dimension $n$, con $(x_1,x_2,\\dots,x_n)$ y $(y_1,y_2,\\dots,y_n)$ sus respectivas coordenadas. Se tiene entonces, que la distancia de Manhattan esta definida como:\n",
        "\n",
        "$$d=|x_1-y_1|+|x_2-y_2|+|x_3-y_3|+\\dots+|x_n-y_n|$$\n",
        "\n",
        "Fuente: [Psychonaut](https://logicplum.com/knowledge-base/manhattan-distance/)\n",
        "\n",
        "En la figura anterior se puede observar la diferencia entre la distancia de Manhattan (líneas amarilla, rojas y azules) y la distancia Euclidiana (línea verde).\n"
      ],
      "metadata": {
        "id": "msSKFOTAKRcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distancia de Mahalanobis\n",
        "La distancia que existe entre dos puntos que se encuentran en un espacio multivariado, relativa al centroide (punto central \"media general\" donde todas las medias de las variables se interceptan).\n",
        "\n",
        "Supongamos que $X_A$ y $X_B$ son objetos  y $C$ es la matriz de covarianzas. Se tiene entonces que la distancia de Mahalanobis es:\n",
        "\n",
        "$$d=[(X_B-X_A)^t\\cdot C^{-1}\\cdot(X_B-X_A)]^{1/2}$$\n",
        "\n",
        "Fuente: [Statistics How To](https://www.statisticshowto.com/mahalanobis-distance/)\n",
        "\n",
        "Se tiene que entre más grande la distancia de Mahalanobis, más lejos se encuentra el dato del centroide. Uno de los usos más importantes de esta distancia es la identificación y obtención de los datos atípicos."
      ],
      "metadata": {
        "id": "wFd3zI3MO0WD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distancia de Minkowski\n",
        "Esta distancia es una generalización de las distancias Manhattan y Euclidiana que se mencionaron anteriormente, en donde se agrega un parámetro $p$. Se tiene que si este parámetro $p$ es $1$, entonces esta distancia es igual a la de Manhattan, pero si se tiene que $p$ es $2$ es igual a la Euclidiana.\n",
        "\n",
        "Supongamos que se tiene $P$ y $Q$ puntos con $(p_1,p_2,\\dots,p_n)$ y $(q_1,q_2,\\dots,q_n)$ sus respectivas coordenadas. Se tiene entonces que la distancia Minkowski esta definida como:\n",
        "$$d=(|p_1-q_1|^p+|p_2-q_2|^p+|p_3-q_3|^p+\\dots+|p_n-q_n|^p)^{1/p}$$\n",
        "\n",
        "Fuente:[OpenGenus](https://iq.opengenus.org/minkowski-distance/)"
      ],
      "metadata": {
        "id": "rijMhXI7ziqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Otras similitudes comunes  "
      ],
      "metadata": {
        "id": "ojkvbexoc9ap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, se presentarán otras dos pseudo-distancias que son muy comunes para la ejecución del algoritmo KNN, las cuales serán útiles para conjuntos de datos en los que se quiera realizar clasificación teniendo en cuenta otro tipo de similitudes entre las unidades de análisis. \n",
        "\n",
        "Reciben el nombre de \"pseudo-distancias\" puesto que suelen no cumplir alguna de las propiedades formales para definir una métrica de distancia. Además, es de importancia resaltar que también existen otras pseudo-distancias que permitien aplicar este método a otras clases de datos de mayor complejidad como series de tiempo o datos multimedia.\n"
      ],
      "metadata": {
        "id": "KAyoGorNdSJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similitud del Coseno\n",
        "\n",
        "Esta es una medida que trabaja con vectores de variables definidas en un espacio vectorial con producto interno. Sin embargo, esta similitud se define con el coseno del ángulo formado entre dos vectores $P$ y $Q$, de tal forma la medida es independiente a la magnitud de los vectores. Dicha independencia permite que la similitud del coseno sea ampliamente usada en la minería de texto, para comparar documentos sin importar su longitud. Los vectores de características serán los conteos de las palabras dentro de cada uno de los textos.\n",
        "\n",
        "Particularmente, suele ser usada en el espacio positivo, por lo cual el resultado está claramente definidio en el intervalo $[0,1]$. Por lo general, el término de la distancia del coseno se refiere al complemento de esta, puesto que se define con la fórmula:\n",
        "\n",
        "$$D_{cos}(P,Q)=1- S_{cos}(P,Q)$$\n",
        "\n",
        "donde $S_{cos}$ corresponde a la similitud del coseno, definida a partir del producto punto como\n",
        "$$S_{cos}(P,Q) = \\frac{\\boldsymbol{P}· \\boldsymbol{Q}}{||\\boldsymbol{P}||||\\boldsymbol{Q}||} =\\frac{\\sum^n_{i=1}p_iq_i}{\\sqrt{\\sum^n_{i=1}p_i^2} \\sqrt{\\sum^n_{i=1}q_i^2}}$$\n",
        "\n",
        "Sin embargo, esta distancia no cumple con la propiedad de la desigualdad triangular.\n",
        "\n",
        "Fuente: [wiki](https://hmong.es/wiki/Cosine_similarity)"
      ],
      "metadata": {
        "id": "TKfbCCThNYGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similitud de la Correlación\n",
        "\n",
        "La construcción de esta distancia es muy parecida a la hecha para definir la distancia del coseno. Puesto que, de forma similar, cuando se habla de la distancia de correlación se hace referencia al complemento de la similitud de correlación entre dos vectores $P$ y $Q$, los cuales corresponderán a las unidades de análisis en la clasificación. Es decir, la distancia de la correlación se define como:\n",
        "\n",
        "$$D_{corr}(P,Q)=1- S_{corr}(P,Q)$$\n",
        "\n",
        "donde $S_{corr}$, en realidad, corresponderá a un coeficiente de correlación. Comúnmente se utiliza o el coeficiente de Pearson, utilizado para variables normalmente distribuidas, o el coeficiente de Spearman, para variables que no tienen distribución normal y por lo tanto se basa en los rangos de los valores.\n",
        "Si usamos el coeficiente de correlación de Pearson $r(P,Q)$, donde $P$ y $Q$ son vistos como las \"variables\" en esta medida, entonces se tiene que: \n",
        "\n",
        "$$S_{corr}(P,Q) = r(P,Q) = \\frac{\\sum^n_{i=1}(p_i-\\bar{p})(q_i-\\bar{q})}{(n-1)s_ps_q}$$\n",
        "\n",
        "Con $\\bar{p}$ y $s_p$ como la media y desviación estándar de los valores en el vector $P$. Este ajuste por la media hace que la similitud sea invariante a cambios de escala entre las características del vector, por lo cual, su aplicación es muy común en contextos donde las características reflejan la asignación de recursos, por ejemplo, el gasto de hogares en distintos productos y servicios."
      ],
      "metadata": {
        "id": "gV-r0pi85Cpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Ejemplos de aplicación "
      ],
      "metadata": {
        "id": "V9F0V0YNdUva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Clásica: \n",
        "Definir bien los métodos en Scikit learn. Especificar que se usa el algoritmo de fuerza bruta, explicando de manera sencilla que es.\n",
        "*   Coseno\n",
        "*   Correlación\n",
        "\n"
      ],
      "metadata": {
        "id": "oXTlGfphdbSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Algoritmos de optimización del método "
      ],
      "metadata": {
        "id": "ho2-a5_pdp_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* K - D tree: reducir distancias que se calculan para que sea más rápido\n",
        "* Ball-tree: mejora K-D tree al funcionar para una alta dimensionalidad"
      ],
      "metadata": {
        "id": "ykEUlm5AfIMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Ejemplos de métodos optimizados "
      ],
      "metadata": {
        "id": "9yHNlaPUg4D0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incluimos reducción de dimensionalidad con knn?"
      ],
      "metadata": {
        "id": "FDJVDsOZg9US"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8) Ejercicio para el lector"
      ],
      "metadata": {
        "id": "mQwb3EBhhqh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9) Ventajas y Desventajas"
      ],
      "metadata": {
        "id": "hjuyVr0QhZKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizar este tipo de algoritmos trae consigo muchas ventajas pero a su vez desventajas. Una de las ventajas más significativa que se tiene es que es no parámetrico, como se mencionó anteriormente, al obtener esto, no se hacen suposiciones explícitas sobre la forma funcional de los datos, por lo que se evitan inconvenientes con la distribución subyacente de estos. \n",
        "\n",
        "De igual manera, se tiene que el algoritmo es simple, puesto que es sencillo de explicar, comprender e interpretar. A pesar de esto, este algoritmo tiene una precisión alta, relativamente. Esto se debe a que no es sensible a los valores atípicos que se puedan encontrar, aún así el ruido o las características irrelevantes pueden llegar a afectar la precisión del mismo. "
      ],
      "metadata": {
        "id": "cG8AQaoR54hy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por otra parte, una de las desventajas de este algoritmo es el coste computacional que se necesita en cuanto a memoria y tiempo para poder realizarlo, esto debido a que se almacenan siempre casi todos los datos de entrenamiento. \n",
        "\n",
        "Otra desventaja es la conocida maldición de la dimensionalidad, la cual implica que el algoritmo no funciona correctamente con entradas de datos de alta dimensionalidad. Esto se debe a que al tener tantas variables puede que algunas no aporten mucha información, lo que hace que la precisión del modelo disminuya.\n",
        "Igualmente, como se presentan muchas características no es tan sencillo hacer clusters y por lo que los algoritmos de distancias se pueden confundir, puesto que al tener tantas variables los puntos pueden parecer equidistantes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XhdEPMeLCjZ6"
      }
    }
  ]
}